\section{Motivation}

More and more people consume media from client-server based sources (e.g., Facebook, Netflix, Spotify, etc.) instead of traditional one-to-many sources (e.g., television, radio, etc.) that usually carry emergency alerts. The current solution to alerting users of client-server systems is to provide an emergency alert feed that websites and providers can periodically poll and then push to users. While television and radio broadcasters (and providers) are legally required to disseminate these messages, websites and ISPs\footnote{Companies that provide both (like AT\&T), only are required to send via the mandated medium} are not.\cite{cfr47}

More importantly, the existing systems treat the internet as a reliable black box, ignoring the increasing frequency and strength of cyber-attacks. In `traditional' emergency scenarios the internet is either present or not; physical cables are down or up. However, in a denial of service attack, the physical infrastructure is intact, but the ability for useful communication is heavily degraded. In a scenario where either terrorist or state actors decide to attack in both the physical \textit{and} cyber realms, citizens still need to be able to receive these critical alerts.

\section{Background}
\subsection{Trotsky}
The foundation for this project is Trotsky, a clean-slate internet framework that separates intra (L3) and inter (L3.5) domain abstractions, and allows for a proliferation of specialized network architectures. The forwarding appliance in Trotsky is the Trotsky Processor (TP): an end-point for an L3.5 pipe that forwards based on the L3.5 protocol agent of the incoming packet. New inter-domain architectures can be developed and deployed by simply adding a routing agent to TPs. In addition, new intra-domain architectures can be created and replaced within a domain without affecting the L3.5 architecture.

We leverage Trotsky both as a way of deployment and as a rationale for our design. Deployment in Trotsky is as simple as pushing a software update to TPs. We also use the L3.5 abstraction of domain-to-domain communication to simplify the problem from sending a message to every end-host to sending a message to every ISP. We aim to solve one specific problem: \textit{emergency} broadcast---with Trotsky this is allowed and even encouraged. There is no reason to add complexity to existing architectures or create a new all-encompassing architecture because Trotsky allows for an ecosystem of specialized architectures. 

\subsection{Existing Infrastructure}\label{infrabckgrnd}
The US public alert system has been around for over half a century\footnote{This paper focuses on the US, but most countries (if they have such systems) have similar designs.}. The system begins with FEMA initiating an alert and sending it to Primary Entry Points (PEPs) across the country via robust RF communication. PEPs are hardened sites that help spread the alert to local radio and TV stations (and can also serve is origin points for local alerts). FEMA (at the same time as the aforementioned broadcast) publishes alerts to an online information feed. All cellular providers and, as seen in a 2018 test of the system, a majority of radio \& television providers use this online feed. Ultimately, the FEMA message first goes to distributors, which then, in turn, deliver the alert to individual citizens. \cite{ipaws101}

The 2018 system test showed some interesting results about the effectiveness of the system. Overall only about 80\% of people received the alert, showing that there is room for improvement. Furthermore, most people that were surveyed received the alert on their smartphone, rather than radio or television (combined less than 10\% of respondents ). The cellular transmission caused a variety of issues, ranging from ensuring service outages, to receipt of dozens of duplicate alerts.\cite{weatest,everbridge}

\section{Design Overview}
Our design combines the abstractions provided by Trotsky with the hierarchical broadcast design provided by the current emergency broadcast infrastructure. Concretely, this means flooding an alert on L3.5, and then having ISPs push the message to citizens. 

\begin{figure}[tp]
\centering
\includegraphics[width=8.5cm]{figures/full_diagram_v2.png}
\caption{Step-by-step process of a nation-wide broadcast}
\label{fig:brdcast}
\end{figure}


\subsection{Detailed Proposed Process}
The steps for a national alert solution using our proposed architecture are shown in Figure \ref{fig:brdcast}, and are described in detail below.

\textbf{1.} Some approved entity sends an out-of-band signed message to the PEPs\footnote{These are the same PEPs mentioned in section \ref{infrabckgrnd}} across the country. This step helps ensure that the message is inserted into multiple points of the internet, reducing the impact of network partition. 

\textbf{2. \& 3.} PEPs confer with each other (out-of-band) to reduce the risk of a compromised PEP (described in section \ref{acl}). 

\textbf{4.} PEPs then send the message to their ISP where the first TP receives the message. The TP ensures that the message has the correct signature (described in section \ref{acl}), dropping the message if it does not.

\textbf{5.} The TP initiates L3.5 flooding\footnote{Continual broadcast to all neighbors.} of the packetized message, encoding the data using a fountain code (described in section \ref{reliable}). Every TP that receives a packet from the message \textit{also checks} the signature before forwarding. If the signature does not match, the packet is dropped.

\textbf{6.} Each TP floods until its neighbor responds with `Message Received.' 

\textbf{7.} When each TP is able to decode the message, the owner of the TP (an ISP) will begin internal, intra-domain broadcast to its customers.

\\
Once messages get to an ISP, the ISP is responsible for the `last-mile' delivery to people. This delivery service can differ domain to domain. For a wireless provider, they may use the same SMS-based emergency alert system that is in use today. ISPs can use multicast, existing modem communication systems, or whatever technologies they may have. Finally, the host-based delivery is as simple as having browsers or OS vendors accepting special, verified `alert' packets. 

A local emergency broadcast would follow a similar procedure, but still differs in two main ways. First, it may not require cross validation with other PEPs. This is because a locality may only have one PEP, and an initiating authority would go directly to that PEP for broadcasting. Second, when the message is flooded on L3.5, TPs in different localities would not continue the broadcast\footnote{If a TP knew its neighbors were in the same location as the broadcast, it would forward the packets.}. 

To draw our discussion of the overall design to a close, we will now discuss how the packet layout enables the functionality of our architecture. The fields in the packet are shown in Figure \ref{fig:pckt}; everything `inside' of the L3.5 header was created as part of our proposal. The L3.5 header specifies that the TP should pass the packet to the Emergency Broadcast processing agent. The\textit{ Broadcast ID} is a  flow number to differentiate between messages. The \textit{Total Number Sent} specifies how many packets the TP should receive before sending `Message Received.' This will be larger than the number of needed for decoding the message and will be described in section \ref{reliable}. \textit{Location} specifies what region\footnote{Any standardized location format works (e.g., ZIP code, city, etc.)} the message is for, enabling local broadcast. The \textit{Crypto Signature} is integral to the verifying that the packet came from an authorized origin and that it has not been tampered with. The \textit{Time Stamp} states when the message was initially sent. Together these two fields prevent replay-based attacks (messages with a valid signature and a stale time stamp will be dropped).
\begin{figure}[tp]
\centering
\includegraphics[width=8.5cm]{figures/packet_header_final.png}
\caption{Packet layout}
\label{fig:pckt}
\end{figure}

\subsection{Rationale}
The goal for this architecture is two-fold: \textbf{1)} robust message delivery and \textbf{2)} access control. We further worked with several key assumptions:  \textbf{A)} emergency broadcasts are relatively infrequent, \textbf{B)} Trotsky is the standard framework, and \textbf{C)} some central authority that must be trusted\footnote{This is meant in both having some sort of centralized key storage and also in the sense of having someone with absolute ability to launch an alert (the president)}. This prioritization led to the design we have here.  

Goal \textbf{1} and assumptions \textbf{B} allowed us to consider in-networks solutions. End-to-end arguments over many lossy links\footnote{While the internet is usually \textit{not} lossy, in congestion-attack scenarios that we are concerned with, packet loss is common.} are weak because the per-link drop-probabilities are compounded and result in low chance of delivery. Trotsky allows us to place functionality in the network, reducing the number of links between logical endpoints, increasing robustness. In network support also allows for extensions like increasing QoS levels for these alerts\footnote{To reiterate: this is an extension.}. 

Goal \textbf{1} and assumption \textbf{A} led us to the idea of flooding. This architecture is used for infrequent and \textit{urgent} events, meaning that degradation of other traffic is tolerable. Using BGP (or any `normal' inter-domain routing) paths increase the probability of message failure if links go down or are unstable; it also introduces unnecessary complexity. The use of many PEPs to introduce the message to the system also reduces the single-point-of-failure of having a truly centralized network entry point.

In the following section, we will focus on goal \textbf{2} and assumption \textbf{C}. 
\section{Access Control}\label{acl}
The goal of access control is crucial to users receiving only critical information in time of emergencies. We limit the ability to broadcast messages on the network to a small group of trusted parties such as local and federal governments. Our network architecture 
\subsection{Overview}
To ensure that only authorized users can broadcast emergency messages on the network, we include a crytopgraphic signature in the header of our L3.5 packet header. 
\subsection{Threat Model}
\subsection{Extensions}


\section{Reliable Transmission}\label{reliable}
In order to increase the speed and reliability of message transmission, we will use RaptorQ fountain codes (we will use RaptorQ and rateless codes interchangeably throughout this paper). The idea of fountain codes is that a finite source of $k$ symbols can be transformed into an arbitrarily long stream of non-redundant symbols. Furthermore, the receiver only needs to receive \textbf{any} $k + \epsilon$ symbols to recover the original message. For RaptorQ, $\epsilon = 0$ and $\epsilon = 2$ give, respectively, a $99\%$ and $99.9999\%$ chance of recovery.\cite{raptorq, raptorqpresent}

RaptorQ encoding also provides high performance through linear time encoding and decoding, reducing the burden on TPs\footnote{Specifically that the encoding/decoding scheme does not rely on specialized hardware}. Codornices, a RaptorQ package from ICSI, recently released performance results. For a 128 kB message on x86, the encode and decode took under $0.5$ ms each, and was able to produce/consume at a rate of $2$ Gbps\cite{raptorqperf}. This performance is definitely sufficient for our application, as will be described in the following section.

\subsection{Use in our system}
In our design we encode a message once: when it arrives at the first TP. All subsequent TPs (in other domains) decode the message (for intra-domain broadcast) and resend the \textit{already encoded} message to neighbors. Since we are reducing a potentially infinite stream to a finite length one we ensure that nodes receive \textit{more} packets than are needed to only reconstruct the message. For a message of $k$ blocks, we set the \textit{Total Number Sent} packet field (see Figure \ref{fig:pckt}) to $k\cdot f_{overshoot}$. This overshoot value is not permanently fixed and can be tuned: a large $f_{overshoot}$ uses more bandwidth, but ensures more packets for future nodes to re-transmit, while a small $f_{overshoot}$ (specifically $1$) is just blind sending of a message.

The choice to not use full RaptorQ decoding and encoding at each step was based on the concern of blocking\footnote{`Blocking' was chosen over `store-and-forward' in order to emphasize that this is on a per \textbf{message} basis}, or waiting for an entire message to arrive before flooding. All the time that one node spends waiting to receive the remainder of a message is time wasted in transmission. We will see this come back up in the following evaluation section.

\section{Evaluation}
For our evaluation we used the python networkx library to model the high level behavior of different propagation modes.\cite{nwx}
For network topologies, we used an abridged Rocketfuel ISP level map. \cite{rocketfuel}

\begin{figure}[tp]
\centering
\includegraphics[width=6cm]{figures/algo_v2.png}
\caption{Basic simulation algorithm used}
\label{algo:a1}
\end{figure}

\subsection{Simulation Design}
The simulator provides as step-based simulation of propagation of packets until all nodes have received the full message. The pseudocode is provided in Figure \ref{algo:a1}. The simulator makes sure that this abstraction of lock-step propagation is maintained (e.g., When on iteration $i$, and node $u$ sends $p_{a+1}$ to $v$, when $v$ gets its turn to send on iteration $i$, \textbf{cannot} send $p_{a+1}$). The simulation stops when all nodes have received the message (this varies based on model being tested). 

We modeled ACK-based senders, rateless senders (decode-only), and blocking rateless senders (encode and decode at each step). To simulate ACK-based sending, each node keeps track of the largest `sequence number` that each neighbors has received. Nodes repeatedly send one packet until the neighbor explicitly tells it to move on. 

For rateless senders, each node sequentially goes through received packets and blindly sends them (there is no knowledge if the receiver has received it). The first node is given $len(MSG)\cdot f_{overshoot}$ packets to begin sending. The simulation ends when all nodes have received any set of $len(MSG)$ distinct packets. The blocking rateless sender cannot transmit until it has received $len(MSG)$ packets, but, unlike the rateless one, it sends an infinite stream of packets.

\subsection{Experimental Setup}
For each experiment we defined a link-based drop probability value, selected a given propagation method and ran the simulation. For each set of control variables we repeated the simulation 100 times to get a stronger sample size. For all experiments, we selected one sender node (at random from the graph) and sent a 15-packet-long message.

We were only concerned with relative performance of methods, not absolute performance. Drops were used to model congestion because routing devices under attack would begin dropping packets once buffers filled up. We measure performance in terms of "Numbers of Iterations," or the amount of time the simulation ran for. Converting from this to actual propagation time would look something along the lines of $Num\_of\_Iterations\cdot \frac{\overline{Transmission\_Latency}}{\overline{Link\_Bandwidth}}$\footnote{We do not intend to treat these as any form of real world estimations, and this equation is just a hypothetical in the right direction}.

\subsection{Results}

\begin{figure}[tp]
\centering
\noindent
\includegraphics[width=8cm]{figures/ACK_FINAL.png}
\caption{Box-and-Whisker plot of the ACKing sender }
\label{graph:ack}
\end{figure}

\begin{figure}[tp]
\centering
\noindent
\includegraphics[width=8cm]{figures/rateless_FINAL.png}
\caption{Box-and-Whisker plot of the rateless sender}
\label{graph:rateless}
\end{figure}


Our results are summarized in Figures \ref{graph:ack}, \ref{graph:rateless},\ref{graph:ackvsrateless}, \ref{graph:ackvsratelessLOG}, \ref{graph:blockingvsnot}, and \ref{graph:varyParams} \footnote{Note that the drop probability of $0.99$ was excluded from Figure \ref{graph:ack} and Figure \ref{graph:rateless} in order to increase readability.}.

The first two graphs show the performance of the ACK-based sender and rateless sender relative to themselves as the probability of packet-loss increases. From these graphs, we can see how the message propagation time would vary based on the severity of packet loss. These graphs provide a stand-alone baseline for each major sending method. It should be noted that y-axis on these graphs are \textbf{10x} greater in Figure \ref{graph:ack} as compared to Figure \ref{graph:rateless}.

Figure \ref{graph:ackvsrateless} compares the average performance of the ACK-based sender and the rateless sender as drop probability increases. The main takeaway from this graph is that the performance of ACK-based sending rapidly degrades when the drop probability approaches $100\%$. To better understand the difference between these two senders at lower drop probabilities, look at Figure \ref{graph:ackvsratelessLOG}, which uses a logarithmic $y$-axis. Even for mild conditions, the rateless sender is almost \textbf{10x} faster. 

The remaining graphs focus on comparisons of rateless senders. Figure \ref{graph:blockingvsnot} shows a direct comparison of the blocking rateless and 'normal' rateless senders. The 

We were initially unsure about whether we should immediately send packets to neighbors or wait for an entire message to begin sending. Figure \ref{graph:blockingvsnot} shows that the version that immediately sends always outperforms the blocking version. The improvement is always present, but becomes especially present when probability of loss increases. It should be noted that the both rateless senders are far better than the ACKing sender.

Overall the results are not surprising. The rateless sender benefits from being able to send for the longest amount of time (even if it is only repeatedly sending one block). The ACK-based sender takes a double hit from lossy links, and thus has a severe performance degradation. The blocking, rateless sender suffers from having to wait longer to begin transmitting; this surprisingly is not that much for all but the most extreme drop rates.

\begin{figure}[tp]
\centering
\noindent
\includegraphics[width=8cm]{figures/ACK_RATELESS_FINAL.png}
\caption{Comparison of the ACKing and rateless senders}
\label{graph:ackvsrateless}
\end{figure}

\begin{figure}[tp]
\centering
\noindent
\includegraphics[width=8cm]{figures/TRIPLE_FINAL.png}
\caption{Comparison of the ACKing and two rateless senders on a log y-axis}
\label{graph:ackvsratelessLOG}
\end{figure}

\begin{figure}[tp]
\centering
\noindent
\includegraphics[width=8cm]{figures/big_font/BLOCKING_RATELESS_BETTER.png}
\caption{Comparison of the blocking and non-blocking rateless senders}
\label{graph:blockingvsnot}
\end{figure}

\begin{figure}[tp]
\centering
\noindent
\includegraphics[width=8cm]{figures/big_font/FACTOR_VARY_BETTER.png}
\caption{Comparison of a Rateless senders with various overshoot comparison. The blocking Rateless sender is included for reference}
\label{graph:varyParams}
\end{figure}

\section{Conclusion}
\textbf{LETS WRITE A REAL CONCLUSION}


One actionable avenue that is not explored in this paper is a way of merging our ideas presented here with the existing internet infrastructure. This path, while most likely less clean and more complex could provide a path to  increasing the robustness of the national broadcast system.


\textbf{Acknowledgements:} We thank Murphy McCauley for helping us get started with this idea. We also are grateful for Nick Weaver, who helped us solidify our security model.


% \begin{figure}[tp]
% \centering
% \includegraphics{figures/mouse}
% \caption{\blindtext}
% \end{figure}
% And we need some citation here\cite{floyd1993random, stoica2001chord}